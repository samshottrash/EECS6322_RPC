{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35ecc5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from utils import load_clip, extract_features, get_text_embeds\n",
    "from sae import train_autoencoder\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "838e6ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"resnet50\": \"RN50\",\n",
    "    \"vit_b16\": \"ViT-B/16\",\n",
    "    \"vit_l14\": \"ViT-L/14\"\n",
    "}\n",
    "\n",
    "clip_model_name = models[\"resnet50\"]\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "#//////////////////////////////"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, preprocess = load_clip(clip_model_name, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cee98e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting image features from local dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 10 examples [00:00, 1549.60 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 36.62 examples/s]\n",
      "Filter: 100%|██████████| 10/10 [00:00<00:00, 16.11 examples/s]\n",
      "Extracting CLIP features: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10/10 [00:00<00:00, 490.93 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image feature extraction complete.\n",
      "Processed dataset contains 10 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Specify the path to your CSV file that contains the local image paths.\n",
    "csv_file = \"cc3m_testing.csv\"  \n",
    "\n",
    "print(\"Extracting image features from local dataset...\")\n",
    "# This function applies the preprocessing and extracts features.\n",
    "processed_dataset = extract_features(csv_file, clip_model, preprocess, device)\n",
    "print(\"Image feature extraction complete.\")\n",
    "print(f\"Processed dataset contains {len(processed_dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236616ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training sparse autoencoder...\")\n",
    "neurons_fired = train_autoencoder(clip_model_name)\n",
    "print(f\"Training complete. Neurons fired: {neurons_fired}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ea1a0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting text embeddings: 100%|██████████| 313/313 [06:25<00:00,  1.23s/it]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:01<00:00, 11574.64 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted text embeddings.\n",
      "Text embeddings shape: torch.Size([20000, 1024])\n"
     ]
    }
   ],
   "source": [
    "# %% [code]\n",
    "# Specify the path to your text file with one text (or caption) per line.\n",
    "text_file = \"20k_vocab.txt\"\n",
    "\n",
    "text_embeddings = get_text_embeds(clip_model, text_file, device)\n",
    "print(\"Extracted text embeddings.\")\n",
    "print(\"Text embeddings shape:\", text_embeddings.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d9f49b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smusa28/miniconda3/envs/eecs6322-rpc/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sparse_autoencoder import (\n",
    "    ActivationResamplerHyperparameters,\n",
    "    AutoencoderHyperparameters,\n",
    "    Hyperparameters,\n",
    "    LossHyperparameters,\n",
    "    Method,\n",
    "    OptimizerHyperparameters,\n",
    "    Parameter,\n",
    "    Pipeline,\n",
    "    PipelineHyperparameters,\n",
    "    SourceDataHyperparameters,\n",
    "    SourceModelHyperparameters,\n",
    "    SweepConfig,\n",
    "    sweep,\n",
    ")\n",
    "\n",
    "from sparse_autoencoder.loss.abstract_loss import AbstractLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d747474",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SourceModelHyperparameters.__init__() missing 2 required positional arguments: 'name' and 'cache_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m sweep_config \u001b[38;5;241m=\u001b[39m SweepConfig(\n\u001b[1;32m      2\u001b[0m     parameters\u001b[38;5;241m=\u001b[39mHyperparameters(\n\u001b[1;32m      3\u001b[0m         loss\u001b[38;5;241m=\u001b[39mLossHyperparameters(\n\u001b[1;32m      4\u001b[0m             l1_coefficient\u001b[38;5;241m=\u001b[39mParameter(values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3e-5\u001b[39m, \u001b[38;5;241m1.5e-4\u001b[39m, \u001b[38;5;241m3e-4\u001b[39m, \u001b[38;5;241m1.5e-3\u001b[39m, \u001b[38;5;241m3e-3\u001b[39m]),\n\u001b[1;32m      5\u001b[0m         ),\n\u001b[1;32m      6\u001b[0m         optimizer\u001b[38;5;241m=\u001b[39mOptimizerHyperparameters(\n\u001b[1;32m      7\u001b[0m             lr\u001b[38;5;241m=\u001b[39mParameter(values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m1e-5\u001b[39m, \u001b[38;5;241m5e-5\u001b[39m, \u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;241m5e-4\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m]),\n\u001b[1;32m      8\u001b[0m         ),\n\u001b[0;32m----> 9\u001b[0m         source_model\u001b[38;5;241m=\u001b[39mSourceModelHyperparameters(\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;66;03m# name=Parameter(\"openai/clip\"), # idk if i should specify the model\u001b[39;00m\n\u001b[1;32m     11\u001b[0m             \u001b[38;5;66;03m# cache_names=Parameter([\"vision_model.encoder.layers.11\"]),  # Extract from last layer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m             hook_dimension\u001b[38;5;241m=\u001b[39mParameter(\u001b[38;5;241m768\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m clip_model_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mViT-B/16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1024\u001b[39m)\n\u001b[1;32m     13\u001b[0m         ),\n\u001b[1;32m     14\u001b[0m         source_data\u001b[38;5;241m=\u001b[39mSourceDataHyperparameters(\n\u001b[1;32m     15\u001b[0m             dataset_path\u001b[38;5;241m=\u001b[39mParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcc3m_clip_features\u001b[39m\u001b[38;5;124m\"\u001b[39m),  \u001b[38;5;66;03m# CC3M dataset\u001b[39;00m\n\u001b[1;32m     16\u001b[0m             context_size\u001b[38;5;241m=\u001b[39mParameter(\u001b[38;5;241m256\u001b[39m),  \u001b[38;5;66;03m# Number of tokens/images to process per batch\u001b[39;00m\n\u001b[1;32m     17\u001b[0m             pre_tokenized\u001b[38;5;241m=\u001b[39mParameter(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),  \u001b[38;5;66;03m# CC3M is not pre-tokenized\u001b[39;00m\n\u001b[1;32m     18\u001b[0m             pre_download\u001b[38;5;241m=\u001b[39mParameter(value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),  \u001b[38;5;66;03m# Stream instead of downloading\u001b[39;00m\n\u001b[1;32m     19\u001b[0m             tokenizer_name\u001b[38;5;241m=\u001b[39mParameter(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m         ),\n\u001b[1;32m     21\u001b[0m         autoencoder\u001b[38;5;241m=\u001b[39mAutoencoderHyperparameters(\n\u001b[1;32m     22\u001b[0m             expansion_factor\u001b[38;5;241m=\u001b[39mParameter(values\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m8\u001b[39m])\n\u001b[1;32m     23\u001b[0m         ),\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;66;03m# num_epochs = Parameter(200),\u001b[39;00m\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;66;03m# resample_interval = Parameter(10)\u001b[39;00m\n\u001b[1;32m     26\u001b[0m     ),\n\u001b[1;32m     27\u001b[0m     method\u001b[38;5;241m=\u001b[39mMethod\u001b[38;5;241m.\u001b[39mRANDOM,\n\u001b[1;32m     28\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: SourceModelHyperparameters.__init__() missing 2 required positional arguments: 'name' and 'cache_names'"
     ]
    }
   ],
   "source": [
    "sweep_config = SweepConfig(\n",
    "    parameters=Hyperparameters(\n",
    "        loss=LossHyperparameters(\n",
    "            l1_coefficient=Parameter(values=[3e-5, 1.5e-4, 3e-4, 1.5e-3, 3e-3]),\n",
    "        ),\n",
    "        optimizer=OptimizerHyperparameters(\n",
    "            lr=Parameter(values=[1e-5, 5e-5, 1e-4, 5e-4, 1e-3]),\n",
    "        ),\n",
    "        source_model=SourceModelHyperparameters(\n",
    "            # name=Parameter(\"openai/clip\"), # idk if i should specify the model\n",
    "            # cache_names=Parameter([\"vision_model.encoder.layers.11\"]),  # Extract from last layer\n",
    "            hook_dimension=Parameter(768 if clip_model_name == \"ViT-B/16\" else 1024)\n",
    "        ),\n",
    "        source_data=SourceDataHyperparameters(\n",
    "            dataset_path=Parameter(\"cc3m_clip_features\"),  # CC3M dataset\n",
    "            context_size=Parameter(256),  # Number of tokens/images to process per batch\n",
    "            pre_tokenized=Parameter(value=False),  # CC3M is not pre-tokenized\n",
    "            pre_download=Parameter(value=False),  # Stream instead of downloading\n",
    "            tokenizer_name=Parameter(\"openai/clip-vit-base-patch32\")\n",
    "        ),\n",
    "        autoencoder=AutoencoderHyperparameters(\n",
    "            expansion_factor=Parameter(values=[2,4,8])\n",
    "        ),\n",
    "        # num_epochs = Parameter(200),\n",
    "        # resample_interval = Parameter(10)\n",
    "    ),\n",
    "    method=Method.RANDOM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4249c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(sweep_config)\n",
    "\n",
    "num_neurons_fired = pipeline.train_autoencoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d1736f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a3149a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import math\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "# Import necessary components from the sparse autoencoder repository.\n",
    "# (Adjust these imports if your installation or repository structure is different.)\n",
    "from sparse_autoencoder import (\n",
    "    ActivationResampler,\n",
    "    AdamWithReset,\n",
    "    L2ReconstructionLoss,\n",
    "    LearnedActivationsL1Loss,\n",
    "    LossReducer,\n",
    "    SparseAutoencoder,\n",
    "    Pipeline,\n",
    ")\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0b6b4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --------- CONFIGURATION (edit these values as needed) ---------\n",
    "# Device configuration:\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f499b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CC3M embeddings from cc3m_clip_features ...\n",
      "Loaded dataset with 10 samples.\n"
     ]
    }
   ],
   "source": [
    "# Load the precomputed CC3M CLIP embeddings dataset.\n",
    "embeddings_dataset_path = \"cc3m_clip_features\"  # folder where embeddings were saved\n",
    "print(f\"Loading CC3M embeddings from {embeddings_dataset_path} ...\")\n",
    "source_dataset = load_from_disk(embeddings_dataset_path)\n",
    "print(f\"Loaded dataset with {len(source_dataset)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed58a719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the input dimension for the autoencoder.\n",
    "# For example, if you used a CLIP RN50 model, the embedding dimension is likely 1024.\n",
    "autoencoder_input_dim = 1024  \n",
    "expansion_factor = 16  # You can adjust this factor\n",
    "n_learned_features = int(autoencoder_input_dim * expansion_factor)\n",
    "n_components = 1  # If you only have one hook/activation source; adjust if you use multiple layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "42cdbf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the Sparse Autoencoder ...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SparseAutoencoder.__init__() got an unexpected keyword argument 'n_input_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --------- Build the Sparse Autoencoder ---------\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating the Sparse Autoencoder ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m autoencoder \u001b[38;5;241m=\u001b[39m SparseAutoencoder(\n\u001b[1;32m      4\u001b[0m     n_input_features\u001b[38;5;241m=\u001b[39mautoencoder_input_dim,\n\u001b[1;32m      5\u001b[0m     n_learned_features\u001b[38;5;241m=\u001b[39mn_learned_features,\n\u001b[1;32m      6\u001b[0m     n_components\u001b[38;5;241m=\u001b[39mn_components,\n\u001b[1;32m      7\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoencoder created.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: SparseAutoencoder.__init__() got an unexpected keyword argument 'n_input_features'"
     ]
    }
   ],
   "source": [
    "# --------- Build the Sparse Autoencoder ---------\n",
    "print(\"Creating the Sparse Autoencoder ...\")\n",
    "autoencoder = SparseAutoencoder(\n",
    "    n_input_features=autoencoder_input_dim,\n",
    "    n_learned_features=n_learned_features,\n",
    "    n_components=n_components,\n",
    ").to(device)\n",
    "print(\"Autoencoder created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cc7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------- Set up the Loss and Optimizer ---------\n",
    "# We combine an L1 loss on the learned activations (for sparsity) with an L2 reconstruction loss.\n",
    "l1_coeff = 1.5e-3  # Adjust as needed\n",
    "loss = LossReducer(\n",
    "    LearnedActivationsL1Loss(l1_coefficient=float(l1_coeff)),\n",
    "    L2ReconstructionLoss(),\n",
    ")\n",
    "print(\"Loss function created.\")\n",
    "\n",
    "# Set up the custom Adam optimizer.\n",
    "lr = 5e-4  # Choose an appropriate learning rate\n",
    "optimizer = AdamWithReset(\n",
    "    params=autoencoder.parameters(),\n",
    "    named_parameters=autoencoder.named_parameters(),\n",
    "    lr=float(lr),\n",
    "    betas=(0.9, 0.999),\n",
    "    eps=1e-8,\n",
    "    weight_decay=0,\n",
    "    has_components_dim=True,\n",
    ")\n",
    "print(\"Optimizer created.\")\n",
    "\n",
    "# --------- Set up the Activation Resampler ---------\n",
    "# This component periodically resamples activations during training.\n",
    "activation_resampler = ActivationResampler(\n",
    "    resample_interval=10,  # e.g., every 10 iterations/epochs (adjust as needed)\n",
    "    n_activations_activity_collate=10,\n",
    "    max_n_resamples=math.inf,\n",
    "    n_learned_features=n_learned_features,\n",
    "    resample_epoch_freq=1,  # Resample every epoch, adjust if necessary\n",
    "    resample_dataset_size=1000,  # Use a subset for resampling; adjust according to dataset size.\n",
    ")\n",
    "print(\"Activation resampler created.\")\n",
    "\n",
    "# --------- Set up the Pipeline ---------\n",
    "# Define a directory for saving checkpoints.\n",
    "checkpoint_directory = Path(\"./sae_checkpoints\")\n",
    "checkpoint_directory.mkdir(exist_ok=True)\n",
    "print(f\"Checkpoints will be saved in {checkpoint_directory}\")\n",
    "\n",
    "# Create the training pipeline.\n",
    "pipeline = Pipeline(\n",
    "    activation_resampler=activation_resampler,\n",
    "    autoencoder=autoencoder,\n",
    "    checkpoint_directory=checkpoint_directory,\n",
    "    loss=loss,\n",
    "    optimizer=optimizer,\n",
    "    device=device,\n",
    "    args=None,  # No additional args passed in this minimal example.\n",
    ")\n",
    "print(\"Pipeline created.\")\n",
    "\n",
    "# --------- Run Training ---------\n",
    "num_epochs = 200  # Set the number of training epochs\n",
    "train_batch_size = 256  # Adjust according to memory and dataset size\n",
    "\n",
    "print(\"Starting training ...\")\n",
    "start_time = time()\n",
    "\n",
    "# For this minimal example, we assume the Pipeline class has a simple training method \n",
    "# that accepts the source dataset, batch size, and number of epochs.\n",
    "pipeline.train_autoencoder(\n",
    "    source_dataset=source_dataset,\n",
    "    num_epochs=num_epochs,\n",
    "    train_batch_size=train_batch_size,\n",
    ")\n",
    "\n",
    "total_time = time() - start_time\n",
    "print(f\"Training complete in {total_time:.2f} seconds.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b161c1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c3fd2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9001216e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs6322-rpc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
