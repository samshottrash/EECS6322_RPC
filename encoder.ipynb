{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import clip\n",
    "from datasets import load_dataset\n",
    "from sparse_autoencoder import (\n",
    "    ActivationResamplerHyperparameters,\n",
    "    AutoencoderHyperparameters,\n",
    "    Hyperparameters,\n",
    "    LossHyperparameters,\n",
    "    Method,\n",
    "    OptimizerHyperparameters,\n",
    "    Parameter,\n",
    "    Pipeline,\n",
    "    PipelineHyperparameters,\n",
    "    SourceDataHyperparameters,\n",
    "    SourceModelHyperparameters,\n",
    "    SweepConfig,\n",
    "    sweep,\n",
    ")\n",
    "\n",
    "from sparse_autoencoder.loss.abstract_loss import AbstractLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "\n",
    "The paper says they discovered concepts the CLIP model has learnt using an SAE, \n",
    "so we first get our pre-trained CLIP models for feature extraction:\n",
    "\n",
    "\"We use CLIP [S16] ResNet-50 [S5], ViT-B/16 [S4], and ViT-L/14 [S4] pre-trained feature extractors from the official repository\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clip.available_models()) # to check if those models are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"resnet50\": \"RN50\",\n",
    "    \"vit_b16\": \"ViT-B/16\",\n",
    "    \"vit_l14\": \"ViT-L/14\"\n",
    "}\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def load_clip(model_name):\n",
    "\n",
    "    model, preprocess = clip.load(model_name, device=device)\n",
    "    \n",
    "    return model, preprocess\n",
    "\n",
    "clip_model_name = models[\"resnet50\"]\n",
    "clip_model, preprocess = load_clip(clip_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use model.encode_image() to get the image features of each img in CC3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image):\n",
    "\n",
    "    # prepare image for feature extraction\n",
    "    # print(\"here\")\n",
    "    # print(image)\n",
    "\n",
    "    image_url = image[\"image_url\"]\n",
    "\n",
    "    try:\n",
    "        response = requests.get(image_url, timeout=5)\n",
    "        response.raise_for_status() # raises if fails\n",
    "        image = Image.open(BytesIO(response.content))\n",
    "        image_input = preprocess(image).unsqueeze(0).to(device)\n",
    "\n",
    "        # Calculate features\n",
    "        with torch.no_grad():\n",
    "            image_features = clip_model.encode_image(image_input)\n",
    "        \n",
    "        return {\"clip_features\": image_features}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {image_url}: {e}\")\n",
    "        return {\"clip_features\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Preprocessing\n",
    "\n",
    "The paper trains the SAE on the CC3M dataset whose images are first processed and gone under a feature extraction process via the CLIP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"conceptual_captions\", split=\"train\")\n",
    "dataset = dataset.map(get_features)\n",
    "\n",
    "# filter out failed images\n",
    "dataset = dataset.filter(lambda x: x[\"clip_features\"] is not None)\n",
    "dataset.save_to_disk(\"cc3m_clip_features\")  # Save processed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset)) # to see if some images made it past filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Setting Hyperparameters\n",
    "\n",
    " The paper trains the SAE with an L_2 reconstruction loss & an L_1 sparsity \n",
    " regularisation. It does this using a hyperparameter lambda_1\n",
    " L1 sparsity coefficient (λ1), whose values are set as {3×10−5, 1.5×10−4, 3×10−4, 1.5×10−3, 3×10−3} for \n",
    " hyperparameter sweeps \n",
    "\n",
    " the paper also performs \"hyperparameter sweeps\n",
    " using a heldout set over the learning rate {1 × 10−5, 5 × 10−5, 1 × 10−4, 5 × 10−4, 1×10−3}\"\n",
    " for the SAE, with learning rate 5 × 10−4 chosen for CLIP ResNet-50 model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = SweepConfig(\n",
    "    parameters=Hyperparameters(\n",
    "        loss=LossHyperparameters(\n",
    "            l1_coefficient=Parameter(values=[3e-5, 1.5e-4, 3e-4, 1.5e-3, 3e-3]),\n",
    "        ),\n",
    "        optimizer=OptimizerHyperparameters(\n",
    "            lr=Parameter(values=[1e-5, 5e-5, 1e-4, 5e-4, 1e-3]),\n",
    "        ),\n",
    "        source_model=SourceModelHyperparameters(\n",
    "            name=Parameter(\"openai/clip\"), # idk if i should specify the model\n",
    "            cache_names=Parameter([\"vision_model.encoder.layers.11\"]),  # Extract from last layer\n",
    "            hook_dimension=Parameter(768 if clip_model_name == \"ViT-B/16\" else 1024)\n",
    "        ),\n",
    "        source_data=SourceDataHyperparameters(\n",
    "            dataset_path=Parameter(\"cc3m_clip_features\"),  # CC3M dataset\n",
    "            context_size=Parameter(256),  # Number of tokens/images to process per batch\n",
    "            pre_tokenized=Parameter(value=False),  # CC3M is not pre-tokenized\n",
    "            pre_download=Parameter(value=False),  # Stream instead of downloading\n",
    "            tokenizer_name=Parameter(\"openai/clip-vit-base-patch32\")\n",
    "        ),\n",
    "        autoencoder=AutoencoderHyperparameters(\n",
    "            expansion_factor=Parameter(values=[2,4,8])\n",
    "        ),\n",
    "        num_epochs = Parameter(200),\n",
    "        resample_interval = Parameter(10)\n",
    "    ),\n",
    "    method=Method.RANDOM,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "\n",
    "The paper uses an L_1 Sparsity loss with an L_2 reconstruction Error, while sparse_autoencoder uses an L_1 absolute loss. So we customize the L_1 loss in the library for our use by extending it as mentioned in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomL1SparsityLoss(AbstractLoss):\n",
    "    def forward(self, input, output):\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(sweep_config)\n",
    "\n",
    "num_neurons_fired = pipeline.train_autoencoder()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs6322-rpc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
